{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision.transforms import Compose\n",
    "import editdistance\n",
    "\n",
    "# detection\n",
    "from detection.unet import UNet\n",
    "from detection.dataset import DetectionDataset\n",
    "import detection.transform\n",
    "import detection.routine\n",
    "# the proper way to do this is relative import, one more nested package and main.py outside the package\n",
    "# will sort this out\n",
    "#sys.path.insert(0, os.path.abspath((os.path.dirname(__file__)) + '/../'))\n",
    "\n",
    "# recognition\n",
    "from recognition.model import RecognitionModel\n",
    "from recognition.dataset import RecognitionDataset\n",
    "import recognition.transform #import Compose, Resize, Pad, Rotate\n",
    "import recognition.routine\n",
    "import recognition.common\n",
    "\n",
    "from utils import get_logger, dice_coeff, dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config hyper parametrs\n",
    "#data_path = \"C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\data\\\\\" #path to the data\n",
    "data_path = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//data//\" \n",
    "epochs = 23 #number of epochs\n",
    "batch_size = 16 #batch size\n",
    "image_size = 256 #input image size\n",
    "lr = 0.0001 #learning rate\n",
    "weight_decay = 5e-4 #weight decay\n",
    "lr_step = 8 #learning rate step\n",
    "lr_gamma = 0.5 #learning rate gamma\n",
    "model = UNet()\n",
    "weight_bce = 0.5 #weight BCE loss\n",
    "load = False #load file model\n",
    "val_split = 0.8 #train/val split\n",
    "#output_dir = \"C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\baseline\\\\temp\\\\\"#dir to save log and models\n",
    "output_dir = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//temp//\"\n",
    "part = 0.01 # config which part of train dataset use\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# TODO: to use move novel arch or/and more lightweight blocks (mobilenet) to enlarge the batch_size\n",
    "# TODO: img_size=256 is rather mediocre, try to optimize network for at least 512\n",
    "if load:\n",
    "    model.load_state_dict(torch.load(load))\n",
    "model = model.to(device)\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger = get_logger(os.path.join(output_dir, 'segmentation_train.log'))\n",
    "logger.info('Start training with params:')\n",
    "logger.info(\"Argument %s: %r\", \"data_path\", data_path)\n",
    "logger.info(\"Argument %s: %r\", \"epochs\", epochs)\n",
    "logger.info(\"Argument %s: %r\", \"batch_size\", batch_size)\n",
    "logger.info(\"Argument %s: %r\", \"image_size\",image_size )\n",
    "logger.info(\"Argument %s: %r\", \"lr\", lr)\n",
    "logger.info(\"Argument %s: %r\", \"weight_decay\",weight_decay )\n",
    "logger.info(\"Argument %s: %r\", \"lr_step\", lr_step)\n",
    "logger.info(\"Argument %s: %r\", \"lr_gamma\",lr_gamma )\n",
    "logger.info(\"Argument %s: %r\", \"weight_bce\", weight_bce)\n",
    "logger.info(\"Argument %s: %r\", \"load\", load)\n",
    "logger.info(\"Argument %s: %r\", \"val_split\", val_split)\n",
    "logger.info(\"Argument %s: %r\", \"output_dir\", output_dir)\n",
    "logger.info('Model type: {}'.format(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# TODO: loss experimentation, fight class imbalance, there're many ways you can tackle this challenge\n",
    "criterion = lambda x, y: (weight_bce * nn.BCELoss()(x, y), (1. - weight_bce) * dice_loss(x, y))\n",
    "# TODO: you can always try on plateau scheduler as a default option\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma) \\\n",
    "    if lr_step > 0 else None\n",
    "\n",
    "# dataset\n",
    "# TODO: to work on transformations a lot, look at albumentations package for inspiration\n",
    "train_transforms = detection.transform.Compose([\n",
    "    detection.transform.Crop(min_size=1 - 1 / 3., min_ratio=1.0, max_ratio=1.0, p=0.5),\n",
    "    detection.transform.Flip(p=0.05),\n",
    "    detection.transform.Pad(max_size=0.6, p=0.25),\n",
    "    detection.transform.Resize(size=(image_size, image_size), keep_aspect=True)\n",
    "])\n",
    "# TODO: don't forget to work class imbalance and data cleansing\n",
    "val_transforms = detection.transform.Resize(size=(image_size, image_size))\n",
    "\n",
    "train_dataset = DetectionDataset(data_path, os.path.join(data_path, 'train_segmentation.json'),\n",
    "                                 transforms=train_transforms, part=part)\n",
    "val_dataset = DetectionDataset(data_path, None, transforms=val_transforms, part=part)\n",
    "\n",
    "# split dataset into train/val, don't try to do this at home ;)\n",
    "train_size = int(len(train_dataset) * val_split)\n",
    "val_dataset.image_names = train_dataset.image_names[train_size:]\n",
    "val_dataset.mask_names = train_dataset.mask_names[train_size:]\n",
    "train_dataset.image_names = train_dataset.image_names[:train_size]\n",
    "train_dataset.mask_names = train_dataset.mask_names[:train_size]\n",
    "\n",
    "# TODO: always work with the data: cleaning, sampling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8,\n",
    "                              shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4,\n",
    "                            shuffle=False, drop_last=False)\n",
    "logger.info('Length of train/val=%d/%d', len(train_dataset), len(val_dataset))\n",
    "logger.info('Number of batches of train/val=%d/%d', len(train_dataloader), len(val_dataloader))\n",
    "\n",
    "try:\n",
    "    detection.routine.train(model, optimizer, criterion, scheduler, epochs, train_dataloader, val_dataloader, saveto=output_dir,\n",
    "          device=device, logger=logger, show_plots=True)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'INTERRUPTED.pth'))\n",
    "    logger.info('Saved interrupt')\n",
    "    sys.exit(0)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config hyper parametrs\n",
    "#data_path = \"C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\data\\\\\" #path to the data\n",
    "data_path = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//data//\" \n",
    "epochs=40 #number of train epochs\n",
    "batch_size=128 #batch size\n",
    "weight_decay=5e-4 #weight_decay\n",
    "lr=1e-4 #lr\n",
    "lr_step=None #lr step\n",
    "lr_gamma=None #lr gamma factor\n",
    "input_wh='320x32' #model input size\n",
    "rnn_dropout=0.1 #rnn dropout p\n",
    "rnn_num_directions=1 #bi\n",
    "augs=0 #degree of geometric augs\n",
    "load=None #pretrained weights\n",
    "val_split=0.8 #train/val split\n",
    "#output_dir = \"C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\baseline\\\\temp\\\\\"#dir to save log and models\n",
    "output_dir = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//temp//\"\n",
    "part = 0.01 # config which part of train dataset use\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RecognitionModel(rnn_dropout, rnn_num_directions)\n",
    "if load is not None:\n",
    "    model.load_state_dict(torch.load(load))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logger = get_logger(os.path.join(output_dir, 'recognition_train.log'))\n",
    "logger.info('Start training with params:')\n",
    "logger.info(\"Argument %s: %r\", \"data_path\", data_path)\n",
    "logger.info(\"Argument %s: %r\", \"epochs\", epochs)\n",
    "logger.info(\"Argument %s: %r\", \"batch_size\", batch_size)\n",
    "logger.info(\"Argument %s: %r\", \"weight_decay\",weight_decay )\n",
    "logger.info(\"Argument %s: %r\", \"lr\", lr)\n",
    "logger.info(\"Argument %s: %r\", \"lr_step\", lr_step)\n",
    "logger.info(\"Argument %s: %r\", \"lr_gamma\",lr_gamma )\n",
    "logger.info(\"Argument %s: %r\", \"input_wh\", input_wh)\n",
    "logger.info(\"Argument %s: %r\", \"rnn_dropout\", rnn_dropout)\n",
    "logger.info(\"Argument %s: %r\", \"rnn_num_directions\", rnn_num_directions)\n",
    "logger.info(\"Argument %s: %r\", \"augs\", augs)\n",
    "logger.info(\"Argument %s: %r\", \"load\", load)\n",
    "logger.info(\"Argument %s: %r\", \"val_split\", val_split)\n",
    "logger.info(\"Argument %s: %r\", \"output_dir\", output_dir)\n",
    "logger.info('Model type: {}'.format(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ctc_loss\n",
    "\n",
    "# TODO: try other optimizers and schedulers\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma) \\\n",
    "    if lr_step is not None else None\n",
    "\n",
    "# dataset\n",
    "w, h = list(map(int, input_wh.split('x')))\n",
    "# TODO: again, augmentations is the key for many tasks\n",
    "train_transforms = recognition.transform.Compose([\n",
    "    recognition.transform.Rotate(max_angle=augs * 7.5, p=0.5),  # 5 -> 7.5\n",
    "    recognition.transform.Pad(max_size=augs / 10, p=0.1),\n",
    "    recognition.transform.Resize(size=(w, h)),\n",
    "])\n",
    "val_transforms = recognition.transform.Resize(size=(w, h))\n",
    "# TODO: don't forget to work on data cleansing\n",
    "train_dataset = RecognitionDataset(data_path, os.path.join(data_path, 'train_recognition.json'),\n",
    "                                   abc=recognition.common.abc, transforms=train_transforms, part=part)\n",
    "val_dataset = RecognitionDataset(data_path, None, abc=recognition.common.abc, transforms=val_transforms,part=part)\n",
    "# split dataset into train/val, don't try to do this at home ;)\n",
    "train_size = int(len(train_dataset) * val_split)\n",
    "val_dataset.image_names = train_dataset.image_names[train_size:]\n",
    "val_dataset.texts = train_dataset.texts[train_size:]\n",
    "train_dataset.image_names = train_dataset.image_names[:train_size]\n",
    "train_dataset.texts = train_dataset.texts[:train_size]\n",
    "\n",
    "# TODO: maybe implement batch_sampler for tackling imbalance, which is obviously huge in many respects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8,\n",
    "                              collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8,\n",
    "                            collate_fn=val_dataset.collate_fn)\n",
    "logger.info('Length of train/val=%d/%d', len(train_dataset), len(val_dataset))\n",
    "logger.info('Number of batches of train/val=%d/%d', len(train_dataloader), len(val_dataloader))\n",
    "\n",
    "try:\n",
    "    recognition.routine.train(model, optimizer, criterion, scheduler, epochs, train_dataloader, val_dataloader, saveto=output_dir,\n",
    "          device=device, logger=logger, show_plots=True)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'INTERRUPTED.pth'))\n",
    "    logger.info('Saved interrupt')\n",
    "    sys.exit(0)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
