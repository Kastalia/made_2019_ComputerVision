{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision.transforms import Compose\n",
    "import editdistance\n",
    "\n",
    "# detection\n",
    "from detection.unet import UNet\n",
    "from detection.maskrcnn import maskrcnn_resnet50_fpn\n",
    "from detection.dataset import DetectionDataset\n",
    "import detection.transform\n",
    "import detection.routine\n",
    "import segmentation_models_pytorch as smp\n",
    "# the proper way to do this is relative import, one more nested package and main.py outside the package\n",
    "# will sort this out\n",
    "#sys.path.insert(0, os.path.abspath((os.path.dirname(__file__)) + '/../'))\n",
    "\n",
    "# recognition\n",
    "from recognition.model import RecognitionModel\n",
    "from recognition.dataset import RecognitionDataset\n",
    "import recognition.transform #import Compose, Resize, Pad, Rotate\n",
    "import recognition.routine\n",
    "import recognition.common\n",
    "\n",
    "from utils import get_logger, dice_coeff, dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config hyper parametrs\n",
    "data_path = \"C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\data\\\\\" #path to the data\n",
    "#data_path = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//data//\" \n",
    "epochs = 13 #number of epochs\n",
    "batch_size = 16 #batch size\n",
    "image_size = 256 #input image size\n",
    "lr = 1e-3 #learning rate\n",
    "weight_decay = 5e-4 #weight decay\n",
    "lr_step = 3 #learning rate step\n",
    "lr_gamma = 0.3 #learning rate gamma\n",
    "#model = UNet()\n",
    "#model = smp.Unet('resnext50_32x4d', encoder_weights='imagenet',classes=2)\n",
    "model = smp.FPN(encoder_name='resnext50_32x4d', encoder_weights='imagenet',classes=2)\n",
    "weight_bce = 0.5 #weight BCE loss\n",
    "load = False #load file model\n",
    "val_split = 0.8 #train/val split\n",
    "output_dir = \"temp\\\\\"#dir to save log and models\n",
    "#output_dir = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//temp//\"\n",
    "part = 1 # config which part of train dataset use\n",
    "detectionFile =  'train_segmentation.json'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# TODO: to use move novel arch or/and more lightweight blocks (mobilenet) to enlarge the batch_size\n",
    "# TODO: img_size=256 is rather mediocre, try to optimize network for at least 512\n",
    "if load:\n",
    "    model.load_state_dict(torch.load(load))\n",
    "model = model.to(device)\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-24 00:19:48 Start training with params:\n",
      "2020-06-24 00:19:48 Argument data_path: 'C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\data\\\\'\n",
      "2020-06-24 00:19:48 Argument epochs: 13\n",
      "2020-06-24 00:19:48 Argument batch_size: 16\n",
      "2020-06-24 00:19:48 Argument image_size: 256\n",
      "2020-06-24 00:19:48 Argument lr: 0.001\n",
      "2020-06-24 00:19:48 Argument weight_decay: 0.0005\n",
      "2020-06-24 00:19:48 Argument lr_step: 3\n",
      "2020-06-24 00:19:48 Argument lr_gamma: 0.3\n",
      "2020-06-24 00:19:48 Argument weight_bce: 0.5\n",
      "2020-06-24 00:19:48 Argument load: False\n",
      "2020-06-24 00:19:48 Argument val_split: 0.8\n",
      "2020-06-24 00:19:48 Argument output_dir: 'temp\\\\'\n",
      "2020-06-24 00:19:48 Model type: FPN\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger = get_logger(os.path.join(output_dir, 'segmentation_train.log'))\n",
    "logger.info('Start training with params:')\n",
    "logger.info(\"Argument %s: %r\", \"data_path\", data_path)\n",
    "logger.info(\"Argument %s: %r\", \"epochs\", epochs)\n",
    "logger.info(\"Argument %s: %r\", \"batch_size\", batch_size)\n",
    "logger.info(\"Argument %s: %r\", \"image_size\",image_size )\n",
    "logger.info(\"Argument %s: %r\", \"lr\", lr)\n",
    "logger.info(\"Argument %s: %r\", \"weight_decay\",weight_decay )\n",
    "logger.info(\"Argument %s: %r\", \"lr_step\", lr_step)\n",
    "logger.info(\"Argument %s: %r\", \"lr_gamma\",lr_gamma )\n",
    "logger.info(\"Argument %s: %r\", \"weight_bce\", weight_bce)\n",
    "logger.info(\"Argument %s: %r\", \"load\", load)\n",
    "logger.info(\"Argument %s: %r\", \"val_split\", val_split)\n",
    "logger.info(\"Argument %s: %r\", \"output_dir\", output_dir)\n",
    "logger.info('Model type: {}'.format(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-24 00:19:48 Length of train/val=20505/5127\n",
      "2020-06-24 00:19:48 Number of batches of train/val=1281/321\n",
      "2020-06-24 00:19:48 Starting epoch 1/13.\n",
      "C:\\Users\\Lisen\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "C:\\Users\\Lisen\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "  0%|                                                                                         | 0/1281 [00:00<?, ?it/s]C:\\Users\\Lisen\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Lisen\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:516: UserWarning: Using a target size (torch.Size([1048576])) that is different to the input size (torch.Size([2097152])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (1048576) != input nelement (2097152)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-adb5e90f5c5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     detection.routine.train(model, optimizer, criterion, scheduler, epochs, train_dataloader, val_dataloader, saveto=output_dir,\n\u001b[1;32m---> 40\u001b[1;33m           device=device, logger=logger, show_plots=True)\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\CV\\02_CarPlatesOCR\\detection\\routine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, optimizer, criterion, scheduler, epochs, train_dataloader, val_dataloader, saveto, device, logger, show_plots)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mmasks_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mbce_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdice_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_masks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbce_val\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdice_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-adb5e90f5c5f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# TODO: loss experimentation, fight class imbalance, there're many ways you can tackle this challenge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweight_bce\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mweight_bce\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# TODO: you can always try on plateau scheduler as a default option\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr_gamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2370\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2371\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[1;32m-> 2372\u001b[1;33m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[0;32m   2373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (1048576) != input nelement (2097152)"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# TODO: loss experimentation, fight class imbalance, there're many ways you can tackle this challenge\n",
    "criterion = lambda x, y: (weight_bce * nn.BCELoss()(x, y), (1. - weight_bce) * dice_loss(x, y))\n",
    "# TODO: you can always try on plateau scheduler as a default option\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma) \\\n",
    "    if lr_step > 0 else None\n",
    "\n",
    "# dataset\n",
    "# TODO: to work on transformations a lot, look at albumentations package for inspiration\n",
    "train_transforms = detection.transform.Compose([\n",
    "    detection.transform.Crop(min_size=1 - 1 / 3., min_ratio=1.0, max_ratio=1.0, p=0.5),\n",
    "    detection.transform.Flip(p=0.05),\n",
    "    detection.transform.Pad(max_size=0.6, p=0.25),\n",
    "    detection.transform.Resize(size=(image_size, image_size), keep_aspect=True)\n",
    "])\n",
    "# TODO: don't forget to work class imbalance and data cleansing\n",
    "val_transforms = detection.transform.Resize(size=(image_size, image_size))\n",
    "\n",
    "train_dataset = DetectionDataset(data_path, os.path.join(data_path, detectionFile),\n",
    "                                 transforms=train_transforms, part=part)\n",
    "val_dataset = DetectionDataset(data_path, None, transforms=val_transforms, part=part)\n",
    "\n",
    "# split dataset into train/val, don't try to do this at home ;)\n",
    "train_size = int(len(train_dataset) * val_split)\n",
    "val_dataset.image_names = train_dataset.image_names[train_size:]\n",
    "val_dataset.mask_names = train_dataset.mask_names[train_size:]\n",
    "train_dataset.image_names = train_dataset.image_names[:train_size]\n",
    "train_dataset.mask_names = train_dataset.mask_names[:train_size]\n",
    "\n",
    "# TODO: always work with the data: cleaning, sampling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8,\n",
    "                              shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4,\n",
    "                            shuffle=False, drop_last=False)\n",
    "logger.info('Length of train/val=%d/%d', len(train_dataset), len(val_dataset))\n",
    "logger.info('Number of batches of train/val=%d/%d', len(train_dataloader), len(val_dataloader))\n",
    "\n",
    "try:\n",
    "    detection.routine.train(model, optimizer, criterion, scheduler, epochs, train_dataloader, val_dataloader, saveto=output_dir,\n",
    "          device=device, logger=logger, show_plots=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info('Saved interrupt')\n",
    "    sys.exit(0)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config hyper parametrs\n",
    "data_path = \"C:\\\\Users\\\\Lisen\\\\Desktop\\\\CV\\\\data\\\\\" #path to the data\n",
    "#data_path = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//data//\" \n",
    "epochs=60 #number of train epochs\n",
    "batch_size=128 #batch size\n",
    "weight_decay=5e-4 #weight_decay\n",
    "lr=1e-3 #lr\n",
    "lr_step=5 #lr step\n",
    "lr_gamma=0.4 #lr gamma factor\n",
    "input_wh='320x32' #model input size\n",
    "rnn_dropout=0.1 #rnn dropout p\n",
    "rnn_num_directions=1 #bi\n",
    "augs=2.25 #degree of geometric augs\n",
    "load=None #pretrained weights\n",
    "val_split=0.8 #train/val split\n",
    "output_dir = \"temp\\\\\"#dir to save log and models\n",
    "#output_dir = \"//home//mayer//LocalRepository//JupyterProjects//MADE_2019_cv//02_CarPlatesOCR//temp//\"\n",
    "part = 1 # config which part of train dataset use\n",
    "recognitionFile = 'train_recognition.json'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RecognitionModel(rnn_dropout, rnn_num_directions)\n",
    "if load is not None:\n",
    "    model.load_state_dict(torch.load(load))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logger = get_logger(os.path.join(output_dir, 'recognition_train.log'))\n",
    "logger.info('Start training with params:')\n",
    "logger.info(\"Argument %s: %r\", \"data_path\", data_path)\n",
    "logger.info(\"Argument %s: %r\", \"epochs\", epochs)\n",
    "logger.info(\"Argument %s: %r\", \"batch_size\", batch_size)\n",
    "logger.info(\"Argument %s: %r\", \"weight_decay\",weight_decay )\n",
    "logger.info(\"Argument %s: %r\", \"lr\", lr)\n",
    "logger.info(\"Argument %s: %r\", \"lr_step\", lr_step)\n",
    "logger.info(\"Argument %s: %r\", \"lr_gamma\",lr_gamma )\n",
    "logger.info(\"Argument %s: %r\", \"input_wh\", input_wh)\n",
    "logger.info(\"Argument %s: %r\", \"rnn_dropout\", rnn_dropout)\n",
    "logger.info(\"Argument %s: %r\", \"rnn_num_directions\", rnn_num_directions)\n",
    "logger.info(\"Argument %s: %r\", \"augs\", augs)\n",
    "logger.info(\"Argument %s: %r\", \"load\", load)\n",
    "logger.info(\"Argument %s: %r\", \"val_split\", val_split)\n",
    "logger.info(\"Argument %s: %r\", \"output_dir\", output_dir)\n",
    "logger.info('Model type: {}'.format(model.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ctc_loss\n",
    "\n",
    "# TODO: try other optimizers and schedulers\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma) \\\n",
    "    if lr_step is not None else None\n",
    "\n",
    "# dataset\n",
    "w, h = list(map(int, input_wh.split('x')))\n",
    "# TODO: again, augmentations is the key for many tasks\n",
    "train_transforms = recognition.transform.Compose([\n",
    "    recognition.transform.Rotate(max_angle=augs * 7.5, p=0.5),  # 5 -> 7.5\n",
    "    recognition.transform.Pad(max_size=augs / 10, p=0.1),\n",
    "    recognition.transform.Resize(size=(w, h)),\n",
    "])\n",
    "val_transforms = recognition.transform.Resize(size=(w, h))\n",
    "# TODO: don't forget to work on data cleansing\n",
    "train_dataset = RecognitionDataset(data_path, os.path.join(data_path, recognitionFile),\n",
    "                                   abc=recognition.common.abc, transforms=train_transforms, part=part)\n",
    "val_dataset = RecognitionDataset(data_path, None, abc=recognition.common.abc, transforms=val_transforms,part=part)\n",
    "# split dataset into train/val, don't try to do this at home ;)\n",
    "train_size = int(len(train_dataset) * val_split)\n",
    "val_dataset.image_names = train_dataset.image_names[train_size:]\n",
    "val_dataset.texts = train_dataset.texts[train_size:]\n",
    "train_dataset.image_names = train_dataset.image_names[:train_size]\n",
    "train_dataset.texts = train_dataset.texts[:train_size]\n",
    "\n",
    "# TODO: maybe implement batch_sampler for tackling imbalance, which is obviously huge in many respects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8,\n",
    "                              collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8,\n",
    "                            collate_fn=val_dataset.collate_fn)\n",
    "logger.info('Length of train/val=%d/%d', len(train_dataset), len(val_dataset))\n",
    "logger.info('Number of batches of train/val=%d/%d', len(train_dataloader), len(val_dataloader))\n",
    "\n",
    "try:\n",
    "    recognition.routine.train(model, optimizer, criterion, scheduler, epochs, train_dataloader, val_dataloader, saveto=output_dir,\n",
    "          device=device, logger=logger, show_plots=True)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'INTERRUPTED.pth'))\n",
    "    logger.info('Saved interrupt')\n",
    "    sys.exit(0)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
